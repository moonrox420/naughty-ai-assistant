# Repository structure

A Restack backend application should be structured as follows:

- src/
    - client.py
    - functions/
        - __init__.py
        - function.py
    - workflows/
        - __init__.py
        - workflow.py
    - services.py
    - schedule_workflow.py
    - pyproject.toml
    - env.example
    - README.md
    - Dockerfile
    - restack_up.py

All these files are mandatory.

### File: audio_transcript/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack
from dataclasses import dataclass

from dotenv import load_dotenv

load_dotenv()

@dataclass
class InputParams:
    file_path: str
    target_language: str

async def main(input: InputParams):
    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-TranscribeTranslateWorkflow"

    run_id = await client.schedule_workflow(
        workflow_name="TranscribeTranslateWorkflow",
        workflow_id=workflow_id,
        input={
            "file_path": input.file_path,
            "target_language": input.target_language
        }
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main(InputParams(file_path="./test.mp3", target_language="Spanish")))

if __name__ == "__main__":
    run_schedule_workflow()


### File: audio_transcript/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)
client = Restack(connection_options)


### File: audio_transcript/src/functions/transcribe_audio.py ###
from restack_ai.function import function, FunctionFailure
from dataclasses import dataclass
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

@dataclass
class TranscribeAudioInput:
    file_path: str

@function.defn()
async def transcribe_audio(input: TranscribeAudioInput):    
    if (os.environ.get("OPENAI_API_KEY") is None):
        raise FunctionFailure("OPENAI_API_KEY is not set", non_retryable=True)
    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

    response = client.audio.transcriptions.create(
        model="whisper-1",
        file=open(input.file_path, "rb")
    )

    return response.text



### File: audio_transcript/src/functions/translate_text.py ###
from restack_ai.function import function, log, FunctionFailure
from dataclasses import dataclass
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

@dataclass
class TranslateTextInput:
    text: str
    target_language: str

@function.defn()
async def translate_text(input: TranslateTextInput):    
    if (os.environ.get("OPENAI_API_KEY") is None):
        raise FunctionFailure("OPENAI_API_KEY is not set", non_retryable=True)
    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant that translates text from one language to another."
            },
            {
                "role": "user",
                "content": f"Translate the following text to {input.target_language}: {input.text}"
            }
        ]
    )

    return response.choices[0].message.content



### File: audio_transcript/src/services.py ###
import asyncio
from src.client import client
from src.workflows.transcribe_translate import TranscribeTranslateWorkflow
from src.functions.transcribe_audio import transcribe_audio
from src.functions.translate_text import translate_text

async def main():
    await asyncio.gather(
        client.start_service(
            workflows=[TranscribeTranslateWorkflow],
            functions=[transcribe_audio, translate_text]
        )
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()


### File: audio_transcript/src/workflows/transcribe_translate.py ###
from restack_ai.workflow import workflow, import_functions, log
from dataclasses import dataclass

with import_functions():
    from src.functions.transcribe_audio import transcribe_audio, TranscribeAudioInput
    from src.functions.translate_text import translate_text, TranslateTextInput

@dataclass
class WorkflowInputParams:
    file_path: str
    target_language: str

@workflow.defn()
class TranscribeTranslateWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        log.info("TranscribeTranslateWorkflow started", input=input)

        transcription = await workflow.step(
            transcribe_audio,
            TranscribeAudioInput(
                file_path=input.file_path,
            ),
        )

        translation = await workflow.step(
            translate_text,
            TranslateTextInput(
                text=transcription,
                target_language=input.target_language,
            ),
        )

        return {
            "transcription": transcription,
            "translation": translation
        }


### File: child_workflows/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-ParentWorkflow"
    runId = await client.schedule_workflow(
        workflow_name="ParentWorkflow",
        workflow_id=workflow_id
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=runId
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: child_workflows/src/client.py ###
from restack_ai import Restack

client = Restack()

### File: child_workflows/src/functions/function.py ###
from restack_ai.function import function, log

@function.defn(name="welcome")
async def welcome(input: str) -> str:
    try:
        log.info("welcome function started", input=input)
        return f"Hello, {input}!"
    except Exception as e:
        log.error("welcome function failed", error=e)
        raise e


### File: child_workflows/src/services.py ###
import asyncio
from src.functions.function import welcome
from src.client import client
from src.workflows.parent import ParentWorkflow
from src.workflows.child import ChildWorkflow

async def main():

    await client.start_service(
        workflows= [ParentWorkflow, ChildWorkflow],
        functions= [welcome]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: child_workflows/src/workflows/child.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log
with import_functions():
    from src.functions.function import welcome

@workflow.defn()
class ChildWorkflow:
    @workflow.run
    async def run(self):
        log.info("ChildWorkflow started")
        result = await workflow.step(welcome, input="world", start_to_close_timeout=timedelta(seconds=120))
        log.info("ChildWorkflow completed", result=result)
        return result




### File: child_workflows/src/workflows/parent.py ###
from restack_ai.workflow import workflow, log, workflow_info

from .child import ChildWorkflow

@workflow.defn()
class ParentWorkflow:
    @workflow.run
    async def run(self):
        

        # use the parent run id to create child workflow ids

        parent_workflow_id = workflow_info().workflow_id

        log.info("Start ChildWorkflow and dont wait for result")

        result = await workflow.child_start(ChildWorkflow, workflow_id=f"{parent_workflow_id}-child-start")

        log.info("Start ChildWorkflow and wait for result")
        result = await workflow.child_execute(ChildWorkflow, workflow_id=f"{parent_workflow_id}-child-execute")
        log.info("ChildWorkflow completed", result=result)
        return result




### File: email_sender/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack
from dataclasses import dataclass
import os
from dotenv import load_dotenv

load_dotenv()

@dataclass
class InputParams:
    email_context: str
    subject: str
    to: str

async def main():
    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-SendEmailWorkflow"
    to_email = os.getenv("TO_EMAIL")
    if not to_email:
        raise Exception("TO_EMAIL environment variable is not set")

    run_id = await client.schedule_workflow(
        workflow_name="SendEmailWorkflow",
        workflow_id=workflow_id,
        input={
            "email_context": "This email should contain a greeting. And telling user we have launched a new AI feature with Restack workflows. Workflows now offer logging and automatic retries when one of its steps fails. Name of user is not provided. You can set as goodbye message on the email just say 'Best regards' or something like that. No need to mention name of user or name of person sending the email.",
            "subject": "Hello from Restack",
            "to": to_email
        }
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()


### File: email_sender/schedule_workflow_failure.py ###
import asyncio
import time
from restack_ai import Restack
from dataclasses import dataclass
import os
from dotenv import load_dotenv

load_dotenv()

@dataclass
class InputParams:
    email_context: str
    subject: str
    to: str

async def main():
    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-SendEmailWorkflow"
    to_email = os.getenv("TO_EMAIL")
    if not to_email:
        raise Exception("TO_EMAIL environment variable is not set")

    run_id = await client.schedule_workflow(
        workflow_name="SendEmailWorkflow",
        workflow_id=workflow_id,
        input={
            "email_context": "This email should contain a greeting. And telling user we have launched a new AI feature with Restack workflows. Workflows now offer logging and automatic retries when one of its steps fails. Name of user is not provided. You can set as goodbye message on the email just say 'Best regards' or something like that. No need to mention name of user or name of person sending the email.",
            "subject": "Hello from Restack",
            "to": to_email,
            "simulate_failure": True
        }
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow_failure():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow_failure()


### File: email_sender/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key
)
client = Restack(connection_options)


### File: email_sender/src/functions/generate_email_content.py ###
from restack_ai.function import function, log, FunctionFailure
from dataclasses import dataclass
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

tries = 0

@dataclass
class GenerateEmailInput:
    email_context: str
    simulate_failure: bool = False

@function.defn()
async def generate_email_content(input: GenerateEmailInput):
    global tries

    if input.simulate_failure and tries == 0:
        tries += 1
        raise FunctionFailure("Simulated failure", non_retryable=False)
    
    if (os.environ.get("OPENAI_API_KEY") is None):
        raise FunctionFailure("OPENAI_API_KEY is not set", non_retryable=True)
    
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant that generates short emails based on the provided context."
            },
            {
                "role": "user",
                "content": f"Generate a short email based on the following context: {input.email_context}"
            }
        ],
        max_tokens=150
    )
    
    return response.choices[0].message.content



### File: email_sender/src/functions/send_email.py ###
import sendgrid
from sendgrid.helpers.mail import Mail
import os
from restack_ai.function import function, FunctionFailure
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv()

@dataclass
class SendEmailInput:
    text: str
    subject: str
    to: str

@function.defn()
async def send_email(input: SendEmailInput) -> None:
    from_email = os.getenv('FROM_EMAIL')
    
    if not from_email:
        raise FunctionFailure('FROM_EMAIL is not set', non_retryable=True)
    
    sendgrid_api_key = os.getenv('SENDGRID_API_KEY')

    if not sendgrid_api_key:
        raise FunctionFailure('SENDGRID_API_KEY is not set', non_retryable=True)
    
    
    message = Mail(
        from_email=from_email,
        to_emails=input.to,
        subject=input.subject,
        plain_text_content=input.text
    )
    
    try:
        sg = sendgrid.SendGridAPIClient(sendgrid_api_key)
        sg.send(message)
    except Exception:
        raise FunctionFailure('Failed to send email', non_retryable=False)


### File: email_sender/src/services.py ###
import asyncio
from src.client import client
from src.workflows.send_email import SendEmailWorkflow
from src.functions.send_email import send_email
from src.functions.generate_email_content import generate_email_content

async def main():
    await asyncio.gather(
        client.start_service(
            workflows=[SendEmailWorkflow],
            functions=[generate_email_content, send_email]
        )
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()


### File: email_sender/src/workflows/send_email.py ###
from restack_ai.workflow import workflow, import_functions, log, RetryPolicy
from dataclasses import dataclass
from datetime import timedelta

with import_functions():
    from src.functions.send_email import send_email, SendEmailInput
    from src.functions.generate_email_content import generate_email_content, GenerateEmailInput

@dataclass
class WorkflowInputParams:
    email_context: str
    subject: str
    to: str
    simulate_failure: bool = False

@workflow.defn()
class SendEmailWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        log.info("SendEmailWorkflow started", input=input)

        text = await workflow.step(
            generate_email_content,
            GenerateEmailInput(
                email_context=input.email_context,
                simulate_failure=input.simulate_failure,
            ),
            retry_policy=RetryPolicy(
                initial_interval=timedelta(seconds=10),
                backoff_coefficient=1,
            ),
        )

        await workflow.step(
            send_email,
            SendEmailInput(
                text=text,
                subject=input.subject,
                to=input.to,
            ),
            start_to_close_timeout=timedelta(seconds=120)
        )

        return 'Email sent successfully'


### File: encryption/schedule_workflow.py ###
import asyncio
import time
from src.client import client
async def main():

    workflow_id = f"{int(time.time() * 1000)}-EncryptedWorkflow"
    run_id = await client.schedule_workflow(
        workflow_name="EncryptedWorkflow",
        workflow_id=workflow_id
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: encryption/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from restack_ai.security import converter
import dataclasses
from .codec import EncryptionCodec

connection_options = CloudConnectionOptions(
    engine_id=os.getenv("RESTACK_ENGINE_ID"),
    api_key=os.getenv("RESTACK_ENGINE_API_KEY"),
    address=os.getenv("RESTACK_ENGINE_ADDRESS"),
    data_converter=dataclasses.replace(converter.default(), payload_codec=EncryptionCodec())
)
client = Restack(connection_options)


### File: encryption/src/codec.py ###
import os
from typing import Iterable, List

from cryptography.hazmat.primitives.ciphers.aead import AESGCM
from restack_ai.security import Payload, PayloadCodec

default_key = b"test-key-test-key-test-key-test!"
default_key_id = "test-key-id"


class EncryptionCodec(PayloadCodec):
    def __init__(self, key_id: str = default_key_id, key: bytes = default_key) -> None:
        super().__init__()
        self.key_id = key_id
        # We are using direct AESGCM to be compatible with samples from
        # TypeScript and Go. Pure Python samples may prefer the higher-level,
        # safer APIs.
        self.encryptor = AESGCM(key)

    async def encode(self, payloads: Iterable[Payload]) -> List[Payload]:
        # We blindly encode all payloads with the key and set the metadata
        # saying which key we used
        return [
            Payload(
                metadata={
                    "encoding": b"binary/encrypted",
                    "encryption-key-id": self.key_id.encode(),
                },
                data=self.encrypt(p.SerializeToString()),
            )
            for p in payloads
        ]

    async def decode(self, payloads: Iterable[Payload]) -> List[Payload]:
        ret: List[Payload] = []
        for p in payloads:
            # Ignore ones w/out our expected encoding
            if p.metadata.get("encoding", b"").decode() != "binary/encrypted":
                ret.append(p)
                continue
            # Confirm our key ID is the same
            key_id = p.metadata.get("encryption-key-id", b"").decode()
            if key_id != self.key_id:
                raise ValueError(
                    f"Unrecognized key ID {key_id}. Current key ID is {self.key_id}."
                )
            # Decrypt and append
            ret.append(Payload.FromString(self.decrypt(p.data)))
        return ret

    def encrypt(self, data: bytes) -> bytes:
        nonce = os.urandom(12)
        return nonce + self.encryptor.encrypt(nonce, data, None)

    def decrypt(self, data: bytes) -> bytes:
        return self.encryptor.decrypt(data[:12], data[12:], None)

### File: encryption/src/codec_server.py ###
from functools import partial
from typing import Awaitable, Callable, Iterable, List

from aiohttp import hdrs, web
from google.protobuf import json_format
from restack_ai.security import Payload, Payloads

from .codec import EncryptionCodec


def build_codec_server() -> web.Application:
    # Cors handler
    async def cors_options(req: web.Request) -> web.Response:
        resp = web.Response()
        if req.headers.get(hdrs.ORIGIN) == "http://localhost:8233":
            resp.headers[hdrs.ACCESS_CONTROL_ALLOW_ORIGIN] = "http://localhost:8233"
            resp.headers[hdrs.ACCESS_CONTROL_ALLOW_METHODS] = "POST"
            resp.headers[hdrs.ACCESS_CONTROL_ALLOW_HEADERS] = "content-type,x-namespace"
        return resp

    # General purpose payloads-to-payloads
    async def apply(
        fn: Callable[[Iterable[Payload]], Awaitable[List[Payload]]], req: web.Request
    ) -> web.Response:
        # Read payloads as JSON
        assert req.content_type == "application/json"
        payloads = json_format.Parse(await req.read(), Payloads())

        # Apply
        payloads = Payloads(payloads=await fn(payloads.payloads))

        # Apply CORS and return JSON
        resp = await cors_options(req)
        resp.content_type = "application/json"
        resp.text = json_format.MessageToJson(payloads)
        return resp

    # Build app
    codec = EncryptionCodec()
    app = web.Application()
    app.add_routes(
        [
            web.post("/encode", partial(apply, codec.encode)),
            web.post("/decode", partial(apply, codec.decode)),
            web.options("/decode", cors_options),
        ]
    )
    return app

def run_codec_server():
    web.run_app(build_codec_server(), host="127.0.0.1", port=8081)

### File: encryption/src/functions/function.py ###
from restack_ai.function import function, log

@function.defn()
async def welcome(input: str) -> str:
    try:
        log.info("welcome function started", input=input)
        return f"Hello, {input}!"
    except Exception as e:
        log.error("welcome function failed", error=e)
        raise e


### File: encryption/src/services.py ###
import asyncio
from src.functions.function import welcome
from src.client import client
from src.workflows.workflow import EncryptedWorkflow

async def main():

    await client.start_service(
        workflows= [EncryptedWorkflow],
        functions= [welcome]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: encryption/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log
with import_functions():
    from src.functions.function import welcome

@workflow.defn()
class EncryptedWorkflow:
    @workflow.run
    async def run(self):
        log.info("EncryptedWorkflow started")
        result = await workflow.step(welcome, input="world", start_to_close_timeout=timedelta(seconds=120))
        log.info("EncryptedWorkflow completed", result=result)
        return result




### File: human_loop/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-HumanLoopWorkflow"
    runId = await client.schedule_workflow(
        workflow_name="HumanLoopWorkflow",
        workflow_id=workflow_id
    )

    await client.send_workflow_event(
        event_name="event_feedback",
        event_input={
            "feedback": "This is a human feedback"
        },
        workflow_id=workflow_id,
        run_id=runId,
    )

    end = await client.send_workflow_event(
        event_name="event_end",
        event_input={
            "end": True
        },
        workflow_id=workflow_id,
        run_id=runId,
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: human_loop/src/client.py ###
from restack_ai import Restack

client = Restack()

### File: human_loop/src/functions/function.py ###
from restack_ai.function import function, log
from dataclasses import dataclass
@dataclass
class InputFeedback:
    feedback: str

@function.defn()
async def goodbye() -> str:
    log.info("goodbye function started")
    return f"Goodbye!"

@function.defn()
async def feedback(input: InputFeedback) -> str:
    log.info("feedback function started", input=input)
    return f"Received feedback: {input.feedback}"


### File: human_loop/src/services.py ###
import asyncio
from src.functions.function import feedback, goodbye
from src.client import client
from src.workflows.workflow import HumanLoopWorkflow

async def main():

    await client.start_service(
        workflows= [HumanLoopWorkflow],
        functions= [feedback, goodbye]
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: human_loop/src/workflows/workflow.py ###
from datetime import timedelta
from restack_ai.workflow import workflow, import_functions, log
from dataclasses import dataclass

with import_functions():
    from src.functions.function import feedback as feedback_function, goodbye, InputFeedback

@dataclass
class Feedback:
    feedback: str

@dataclass
class End:
    end: bool

@workflow.defn()
class HumanLoopWorkflow:
    def __init__(self) -> None:
        self.end_workflow = False
        self.feedbacks = []
    @workflow.event
    async def event_feedback(self, feedback: Feedback) -> Feedback:
        result = await workflow.step(feedback_function, InputFeedback(feedback.feedback), start_to_close_timeout=timedelta(seconds=120))
        log.info("Received feedback", result=result)
        return result
    
    @workflow.event
    async def event_end(self, end: End) -> End:
        log.info("Received end", end=end)
        self.end_workflow = end.end
        return end

    @workflow.run
    async def run(self):
        await workflow.condition(
            lambda: self.end_workflow
        )
        result = await workflow.step(goodbye, start_to_close_timeout=timedelta(seconds=120))
        log.info("Workflow ended", result=result)
        return result




### File: openai_greet/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack
from dataclasses import dataclass

@dataclass
class InputParams:
    name: str

async def main():
    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-OpenaiGreetWorkflow"
    runId = await client.schedule_workflow(
        workflow_name="OpenaiGreetWorkflow",
        workflow_id=workflow_id,
        input=InputParams(name="Restack AI SDK User")
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=runId
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: openai_greet/src/client.py ###
from restack_ai import Restack

client = Restack()

### File: openai_greet/src/functions/function.py ###
from restack_ai.function import function, log
from openai import OpenAI
from dataclasses import dataclass
import os
from dotenv import load_dotenv

load_dotenv()

@dataclass
class FunctionInputParams:
    user_content: str
    system_content: str | None = None
    model: str | None = None

@function.defn()
async def openai_greet(input: FunctionInputParams) -> str:
    try:
        log.info("openai_greet function started", input=input)
        client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

        messages = []
        if input.system_content:
            messages.append({"role": "system", "content": input.system_content})
        messages.append({"role": "user", "content": input.user_content})

        response = client.chat.completions.create(
            model=input.model or "gpt-4o-mini",
            messages=messages,
            response_format={
                "json_schema": {
                    "name": "greet",
                    "description": "Greet a person",
                    "schema": {
                        "type": "object",
                        "properties": {
                            "message": {"type": "string"}
                        },
                        "required": ["message"]
                    }
                },
                "type": "json_schema",
            },
        )
        log.info("openai_greet function completed", response=response)
        return response.choices[0].message.content
    except Exception as e:
        log.error("openai_greet function failed", error=e)
        raise e


### File: openai_greet/src/services.py ###
import asyncio
from src.functions.function import openai_greet
from src.client import client
from src.workflows.openai_greet import OpenaiGreetWorkflow
async def main():

    await client.start_service(
        workflows= [OpenaiGreetWorkflow],
        functions= [openai_greet],
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: openai_greet/src/workflows/openai_greet.py ###
from restack_ai.workflow import workflow, import_functions, log
from dataclasses import dataclass
from datetime import timedelta

with import_functions():
    from src.functions.function import openai_greet, FunctionInputParams

@dataclass
class WorkflowInputParams:
    name: str

@workflow.defn()
class OpenaiGreetWorkflow:
    @workflow.run
    async def run(self, input: WorkflowInputParams):
        log.info("OpenaiGreetWorkflow started", input=input)
        user_content = f"Greet this person {input.name}"


        greet_message = await workflow.step(
            openai_greet,
            FunctionInputParams(
                user_content=user_content,
            ),
            start_to_close_timeout=timedelta(seconds=120)
        )
        log.info("OpenaiGreetWorkflow completed", greet_message=greet_message)
        return greet_message


### File: pdf_pydantic/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  
# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key,
)
client = Restack(connection_options)

### File: pdf_pydantic/src/functions/openai_chat.py ###
from pydantic import BaseModel
from restack_ai.function import function, log, FunctionFailure
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

class OpenAiChatInput(BaseModel):
    user_content: str
    system_content: str | None = None
    model: str | None = None

@function.defn()
async def openai_chat(input: OpenAiChatInput) -> str:
    try:
        log.info("openai_chat function started", input=input)

        
        if (os.environ.get("OPENAI_API_KEY") is None):
            raise FunctionFailure("OPENAI_API_KEY is not set", non_retryable=True)
    
        client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

        messages = []
        if input.system_content:
            messages.append({"role": "system", "content": input.system_content})
        messages.append({"role": "user", "content": input.user_content})

        response = client.chat.completions.create(
            model=input.model or "gpt-4o-mini",
            messages=messages
        )
        log.info("openai_chat function completed", response=response)
        return response.choices[0].message.content
    except Exception as e:
        log.error("openai_chat function failed", error=e)
        raise e


### File: pdf_pydantic/src/functions/torch_ocr.py ###
import base64
import io
from typing import Any, List

import numpy as np
from doctr.io import DocumentFile
from fastapi import UploadFile
from numpy.typing import NDArray
from PIL import Image
from pydantic import BaseModel, Field
from restack_ai.function import function
from doctr.models import ocr_predictor

class OCRPrediction(BaseModel):
    pages: List[dict[str, Any]] = Field(
        description="List of pages with OCR predictions"
    )

class OcrInput(BaseModel):
    file_type: str
    file_binary:str

@function.defn()
async def torch_ocr(input: OcrInput) -> str:
    try:
        service = DocumentExtractionService()
        content = base64.b64decode(input.file_binary)

        if input.file_type == "application/pdf":
            doc = DocumentFile.from_pdf(content)
        elif input.file_type.startswith("image/"):
            image: Image.Image = Image.open(io.BytesIO(content))
            processed_img: NDArray[np.uint8] = service._preprocess_image(image)
            doc = DocumentFile.from_images(processed_img)
        else:
            raise ValueError("Unsupported file type")

        result = service.predictor(doc)
        json_output = OCRPrediction.model_validate(result.export())
        return service._process_predictions(json_output)

    except Exception as e:
        raise ValueError(f"Failed to process file: {str(e)}")

class DocumentExtractionService:
    def __init__(self) -> None:
        self.predictor = ocr_predictor(
            det_arch="db_resnet50",
            reco_arch="crnn_vgg16_bn",
            pretrained=True,
            assume_straight_pages=False,
        )

    async def extract(self, file: UploadFile) -> str:
        try:
            content: bytes = await file.read()

            if file.content_type is None:
                raise ValueError("File content type is not available")

            if file.content_type == "application/pdf":
                doc = DocumentFile.from_pdf(content)
            elif file.content_type.startswith("image/"):
                image: Image.Image = Image.open(io.BytesIO(content))
                processed_img: NDArray[np.uint8] = self._preprocess_image(image)
                doc = DocumentFile.from_images(processed_img)
            else:
                raise ValueError("Unsupported file type")

            result = self.predictor(doc)
            json_output = OCRPrediction.model_validate(result.export())
            return self._process_predictions(json_output)

        except Exception as e:
            raise ValueError(f"Failed to process file: {str(e)}")

    def _preprocess_image(self, image: Image.Image) -> NDArray[np.uint8]:
        if image.mode != "RGB":
            image = image.convert("RGB")

        img_array: NDArray[np.uint8] = np.array(image)
        p2, p98 = np.percentile(img_array, (2, 98))
        img_array = np.clip(img_array, p2, p98)
        img_array = ((img_array - p2) / (p98 - p2) * 255).astype(np.uint8)

        return img_array

    def _process_predictions(
        self, json_output: OCRPrediction, confidence_threshold: float = 0.3
    ) -> str:
        processed_text: List[str] = []

        for page in json_output.pages:
            page_text: List[str] = []
            for block in page["blocks"]:
                for line in block["lines"]:
                    line_text: List[str] = []
                    for word in line["words"]:
                        if word["confidence"] > confidence_threshold:
                            line_text.append(word["value"])
                    if line_text:
                        page_text.append(" ".join(line_text))
            processed_text.append("\n".join(page_text))

        return "\n\n=== PAGE BREAK ===\n\n".join(processed_text)

### File: pdf_pydantic/src/main.py ###
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

import time
from src.workflows.pdf import PdfWorkflowInput
from src.workflows.files import FilesWorkflowInput
from src.client import client
import uvicorn
from src.services import run_services
import threading
from fastapi import FastAPI, File, UploadFile
import base64
import asyncio
import signal

app = FastAPI(
    title="PDF Pydantic Example",
    description="An API for scheduling and managing workflows to OCR PDFs and get a summary with OpenaI.",
    version="1.0.0",
    docs_url="/",
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust this in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/api/pdfs")
async def schedule_workflow(files: list[UploadFile] = File(...)):
    try:
        async def process_file(file: UploadFile):
            file_content = await file.read()
            encoded_file = base64.b64encode(file_content).decode('utf-8')
            workflow_id = f"{int(time.time() * 1000)}-PdfWorkflow"
            
            runId = await client.schedule_workflow(
                workflow_name="PdfWorkflow",
                workflow_id=workflow_id,
                input=PdfWorkflowInput(file_type=file.content_type, file_binary=encoded_file)
            )
            
            result = await client.get_workflow_result(
                workflow_id=workflow_id,
                run_id=runId
            )
            return {"file_name": file.filename, "result": result}

        results = await asyncio.gather(*(process_file(file) for file in files))
        
        return {"results": results}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.post("/api/pdfs-parent")
async def execute_files_workflow(files: list[UploadFile] = File(...)):
    try:
        # Read and encode all files
        encoded_files = []
        for file in files:
            file_content = await file.read()
            encoded_file = base64.b64encode(file_content).decode('utf-8')
            encoded_files.append({"file_type": file.content_type, "file_binary": encoded_file})

        # Create a unique workflow ID
        workflow_id = f"{int(time.time() * 1000)}-FilesWorkflow"

        # Schedule the FilesWorkflow with all encoded files
        runId = await client.schedule_workflow(
            workflow_name="FilesWorkflow",
            workflow_id=workflow_id,
            input=FilesWorkflowInput(files=encoded_files)
        )

        # Get the result of the workflow
        result = await client.get_workflow_result(
            workflow_id=workflow_id,
            run_id=runId
        )

        return {"workflow_id": workflow_id, "result": result}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
    
stop_event = threading.Event()

def start_services():
    run_services()

def run_main():
    # Start run_services in a separate thread
    service_thread = threading.Thread(target=start_services)
    service_thread.start()
    
    # Define a signal handler to stop the application
    def signal_handler(sig, frame):
        print("Signal received, stopping services...")
        stop_event.set()
        service_thread.join()  # Wait for the service thread to finish
        print("Services stopped. Exiting application.")
        exit(0)

    # Register the signal handler for SIGINT (Ctrl+C)
    signal.signal(signal.SIGINT, signal_handler)
    
    # Run the FastAPI application with hot reloading
    uvicorn.run("src.main:app", host="127.0.0.1", port=8000, reload=True)

if __name__ == '__main__':
    run_main()

### File: pdf_pydantic/src/services.py ###
import asyncio
from src.functions.torch_ocr import torch_ocr
from src.functions.openai_chat import openai_chat
from src.client import client
from src.workflows.pdf import PdfWorkflow
from src.workflows.files import FilesWorkflow

async def main():

    await asyncio.gather(
      await client.start_service(
          workflows= [PdfWorkflow, FilesWorkflow],
          functions= [torch_ocr, openai_chat]
      )
    )
    
def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()

### File: pdf_pydantic/src/workflows/files.py ###
from restack_ai.workflow import workflow, log, workflow_info
from typing import List
from pydantic import BaseModel
import asyncio
from .pdf import PdfWorkflow, PdfWorkflowInput


class FilesWorkflowInput(BaseModel):
    files: List[PdfWorkflowInput]

@workflow.defn()
class FilesWorkflow:
    @workflow.run
    async def run(self, input: FilesWorkflowInput):
        tasks = []
        parent_workflow_id = workflow_info().workflow_id

        for index, pdf_input in enumerate(input.files, start=1):
            log.info(f"Queue PdfWorkflow {index} for execution")
            # Ensure child workflows are started and return an awaitable
            task = workflow.child_execute(
                PdfWorkflow, 
                workflow_id=f"{parent_workflow_id}-pdf-{index}",
                input=pdf_input
            )
            # Wrap the task in an asyncio.ensure_future to ensure it's awaitable
            tasks.append(asyncio.ensure_future(task))

        # Await all tasks at once to run them in parallel
        results = await asyncio.gather(*tasks)

        for i, result in enumerate(results, start=1):
            log.info(f"PdfWorkflow {i} completed", result=result)

        return {
            "results": results
        }


### File: pdf_pydantic/src/workflows/pdf.py ###
from restack_ai.workflow import workflow, import_functions, log
from datetime import timedelta
from pydantic import BaseModel

with import_functions():
    from src.functions.torch_ocr import torch_ocr, OcrInput
    from src.functions.openai_chat import openai_chat, OpenAiChatInput

class PdfWorkflowInput(BaseModel):
    file_type: str
    file_binary:str

@workflow.defn()
class PdfWorkflow:
    @workflow.run
    async def run(self, input: PdfWorkflowInput):
        log.info("PdfWorkflow started")

        ocr_result = await workflow.step(
            torch_ocr,
            OcrInput(
                file_type=input.file_type,
                file_binary=input.file_binary
            ),
            start_to_close_timeout=timedelta(seconds=120)
        )

        llm_result = await workflow.step(
            openai_chat,
            OpenAiChatInput(
                user_content=f"Make a summary of that PDF. Here is the OCR result: {ocr_result}",
                model="gpt-4o-mini"
            ),
            start_to_close_timeout=timedelta(seconds=120)
        )

        log.info("PdfWorkflow completed")
        return llm_result


### File: production_demo/schedule_interval.py ###
import asyncio
import time
from restack_ai import Restack
from restack_ai.restack import ScheduleSpec, ScheduleIntervalSpec
from datetime import timedelta

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-ChildWorkflow"
    await client.schedule_workflow(
        workflow_name="ChildWorkflow",
        workflow_id=workflow_id,
        schedule=ScheduleSpec(
            intervals=[ScheduleIntervalSpec(
                every=timedelta(seconds=1)
            )]
        )
    )

    exit(0)

def run_schedule_scale():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_scale()

### File: production_demo/schedule_scale.py ###
import asyncio
import time
from restack_ai import Restack

from src.workflows.workflow import ExampleWorkflowInput

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-ExampleWorkflow"
    await client.schedule_workflow(
        workflow_name="ExampleWorkflow",
        workflow_id=workflow_id,
        input=ExampleWorkflowInput(amount=50)
    )

    exit(0)

def run_schedule_scale():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_scale()

### File: production_demo/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-ChildWorkflow"
    run_id = await client.schedule_workflow(
        workflow_name="ChildWorkflow",
        workflow_id=workflow_id
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: production_demo/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  

# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")
api_address = os.getenv("RESTACK_ENGINE_API_ADDRESS")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key,
    api_address=api_address
)
client = Restack(connection_options)


### File: production_demo/src/functions/evaluate.py ###
from restack_ai.function import function, FunctionFailure, log
from openai import OpenAI
from pydantic import BaseModel

class EvaluateInput(BaseModel):
    generated_text: str

@function.defn()
async def llm_evaluate(input: EvaluateInput) -> str:
    try:
        client = OpenAI(base_url="http://192.168.205.1:1234/v1/",api_key="llmstudio")
    except Exception as e:
        log.error(f"Failed to create LLM client {e}")
        raise FunctionFailure(f"Failed to create OpenAI client {e}", non_retryable=True) from e

    prompt = (
        f"Evaluate the following joke for humor, creativity, and originality. "
        f"Provide a score out of 10 for each category for your score.\n\n"
        f"Joke: {input.generated_text}\n\n"
        f"Response format:\n"
        f"Humor: [score]/10"
        f"Creativity: [score]/10"
        f"Originality: [score]/10"
        f"Average score: [score]/10"
        f"Only answer with the scores"
    )

    try:
        response = client.chat.completions.create(
            model="llama-3.2-3b-instruct",
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.5,
        )
    
    except Exception as e:
        log.error(f"Failed to generate {e}")
    
    return response.choices[0].message.content

### File: production_demo/src/functions/function.py ###
from restack_ai.function import function, log, FunctionFailure

tries = 0

from pydantic import BaseModel

class ExampleFunctionInput(BaseModel):
    name: str

@function.defn()
async def example_function(input: ExampleFunctionInput) -> str:
    try:
        global tries

        if tries == 0:
            tries += 1
            raise FunctionFailure(message="Simulated failure", non_retryable=False)
      
        log.info("example function started", input=input)
        return f"Hello, {input.name}!"
    except Exception as e:
        log.error("example function failed", error=e)
        raise e


### File: production_demo/src/functions/generate.py ###
from restack_ai.function import function, FunctionFailure, log
from openai import OpenAI

from pydantic import BaseModel

class GenerateInput(BaseModel):
    prompt: str

@function.defn()
async def llm_generate(input: GenerateInput) -> str:

    try:
        client = OpenAI(base_url="http://192.168.205.1:1234/v1/",api_key="llmstudio")
    except Exception as e:
        log.error(f"Failed to create LLM client {e}")
        raise FunctionFailure(f"Failed to create OpenAI client {e}", non_retryable=True) from e

    try:
        response = client.chat.completions.create(
            model="llama-3.2-3b-instruct",
            messages=[
                {
                    "role": "user",
                    "content": input.prompt
                }
            ],
            temperature=0.5,
        )
    
    except Exception as e:
        log.error(f"Failed to generate {e}")
    
    return response.choices[0].message.content



### File: production_demo/src/services.py ###
import asyncio
import os
from watchfiles import run_process

from src.client import client
from restack_ai.restack import ServiceOptions

from src.functions.function import example_function
from src.functions.generate import llm_generate
from src.functions.evaluate import llm_evaluate

from src.workflows.workflow import ExampleWorkflow, ChildWorkflow
import webbrowser


async def main():

    await asyncio.gather(
        client.start_service(
            workflows=[ExampleWorkflow, ChildWorkflow],
            functions=[example_function],
            options=ServiceOptions(
                max_concurrent_workflow_runs=1000
            )

        ),
        client.start_service(
            task_queue="llm",
            functions=[llm_generate, llm_evaluate],
            options=ServiceOptions(
                rate_limit=1,
                max_concurrent_function_runs=1
            )
        )
    )

def run_services():
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Service interrupted by user. Exiting gracefully.")

def watch_services():
    watch_path = os.getcwd()
    print(f"Watching {watch_path} and its subdirectories for changes...")
    webbrowser.open("http://localhost:5233")
    run_process(watch_path, recursive=True, target=run_services)

if __name__ == "__main__":
    run_services()

### File: production_demo/src/workflows/child.py ###
from datetime import timedelta
from pydantic import BaseModel, Field
from restack_ai.workflow import workflow, import_functions, log

with import_functions():
    from src.functions.function import example_function, ExampleFunctionInput
    from src.functions.generate import llm_generate, GenerateInput
    from src.functions.evaluate import llm_evaluate, EvaluateInput

class ChildWorkflowInput(BaseModel):
    prompt: str = Field(default="Generate a random joke in max 20 words.")

@workflow.defn()
class ChildWorkflow:
    @workflow.run
    async def run(self, input: ChildWorkflowInput):
        log.info("ChildWorkflow started")
        await workflow.step(example_function, input=ExampleFunctionInput(name='John Doe'), start_to_close_timeout=timedelta(minutes=2))

        await workflow.sleep(1)

        generated_text = await workflow.step(
            llm_generate,
            GenerateInput(prompt=input.prompt),
            task_queue="llm",
            start_to_close_timeout=timedelta(minutes=2)
        )

        evaluation = await workflow.step(
            llm_evaluate,
            EvaluateInput(generated_text=generated_text),
            task_queue="llm",
            start_to_close_timeout=timedelta(minutes=5)
        )

        return {
            "generated_text": generated_text,
            "evaluation": evaluation
        }




### File: production_demo/src/workflows/workflow.py ###
import asyncio
from datetime import timedelta
from pydantic import BaseModel, Field
from restack_ai.workflow import workflow, log, workflow_info, import_functions
from .child import ChildWorkflow, ChildWorkflowInput

with import_functions():
    from src.functions.generate import llm_generate, GenerateInput

class ExampleWorkflowInput(BaseModel):
    amount: int = Field(default=50)

@workflow.defn()
class ExampleWorkflow:
    @workflow.run
    async def run(self, input: ExampleWorkflowInput):
        # use the parent run id to create child workflow ids
        parent_workflow_id = workflow_info().workflow_id

        tasks = []
        for i in range(input.amount):
            log.info(f"Queue ChildWorkflow {i+1} for execution")
            task = workflow.child_execute(
                ChildWorkflow, 
                workflow_id=f"{parent_workflow_id}-child-execute-{i+1}",
                input=ChildWorkflowInput(name=f"child workflow {i+1}")
            )
            tasks.append(task)

        # Run all child workflows in parallel and wait for their results
        results = await asyncio.gather(*tasks)

        for i, result in enumerate(results, start=1):
            log.info(f"ChildWorkflow {i} completed", result=result)

        generated_text = await workflow.step(
            llm_generate,
            GenerateInput(prompt=f"Give me the top 3 unique jokes according to the results. {results}"),
            task_queue="llm",
            start_to_close_timeout=timedelta(minutes=2)
        )

        return {
            "top_jokes": generated_text,
            "results": results
        }



### File: quickstart/schedule_calendar.py ###
import asyncio
import time
from restack_ai import Restack
from restack_ai.restack import ScheduleSpec, ScheduleCalendarSpec, ScheduleRange
from src.workflows.workflow import GreetingWorkflowInput
async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-GreetingWorkflow"
    await client.schedule_workflow(
        workflow_name="GreetingWorkflow",
        workflow_id=workflow_id,
        input=GreetingWorkflowInput(name="Bob"),
        schedule=ScheduleSpec(
            calendars=[ScheduleCalendarSpec(
                day_of_week=[ScheduleRange(start=1)],
                hour=[ScheduleRange(start=9)]
            )]
        )
    )

    exit(0)

def run_schedule_calendar():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_calendar()

### File: quickstart/schedule_interval.py ###
import asyncio
import time
from restack_ai import Restack
from restack_ai.restack import ScheduleSpec, ScheduleIntervalSpec
from datetime import timedelta
from src.workflows.workflow import GreetingWorkflowInput
async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-GreetingWorkflow"
    await client.schedule_workflow(
        workflow_name="GreetingWorkflow",
        workflow_id=workflow_id,
        input=GreetingWorkflowInput(name="Bob"),
        schedule=ScheduleSpec(
            intervals=[ScheduleIntervalSpec(
                every=timedelta(minutes=10)
            )]
        )
    )

    exit(0)

def run_schedule_interval():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_interval()

### File: quickstart/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack
from src.workflows.workflow import GreetingWorkflowInput
async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-GreetingWorkflow"
    run_id = await client.schedule_workflow(
        workflow_name="GreetingWorkflow",
        workflow_id=workflow_id,
        input=GreetingWorkflowInput(name="Bob")
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=run_id
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: quickstart/src/client.py ###
import os
from restack_ai import Restack
from restack_ai.restack import CloudConnectionOptions
from dotenv import load_dotenv  
# Load environment variables from a .env file
load_dotenv()


engine_id = os.getenv("RESTACK_ENGINE_ID")
address = os.getenv("RESTACK_ENGINE_ADDRESS")
api_key = os.getenv("RESTACK_ENGINE_API_KEY")
api_address = os.getenv("RESTACK_ENGINE_API_ADDRESS")

connection_options = CloudConnectionOptions(
    engine_id=engine_id,
    address=address,
    api_key=api_key,
    api_address=api_address
)
client = Restack(connection_options)

### File: quickstart/src/functions/function.py ###
from restack_ai.function import function, log
from pydantic import BaseModel

class WelcomeInput(BaseModel):
    name: str

@function.defn()
async def welcome(input: WelcomeInput) -> str:
    try:
        log.info("welcome function started", input=input)
        return f"Hello, {input.name}!"
    except Exception as e:
        log.error("welcome function failed", error=e)
        raise e


### File: quickstart/src/services.py ###
import asyncio
import os
from src.functions.function import welcome
from src.client import client
from src.workflows.workflow import GreetingWorkflow
from watchfiles import run_process
import webbrowser

async def main():

    await client.start_service(
        workflows=[GreetingWorkflow],
        functions=[welcome]
    )

def run_services():
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Service interrupted by user. Exiting gracefully.")

def watch_services():
    watch_path = os.getcwd()
    print(f"Watching {watch_path} and its subdirectories for changes...")
    webbrowser.open("http://localhost:5233")
    run_process(watch_path, recursive=True, target=run_services)

if __name__ == "__main__":
       run_services()


### File: quickstart/src/workflows/workflow.py ###
from datetime import timedelta
from pydantic import BaseModel, Field
from restack_ai.workflow import workflow, import_functions, log
with import_functions():
    from src.functions.function import welcome, WelcomeInput

class GreetingWorkflowInput(BaseModel):
    name: str = Field(default='Bob')

@workflow.defn()
class GreetingWorkflow:
    @workflow.run
    async def run(self, input: GreetingWorkflowInput):
        log.info("GreetingWorkflow started")
        result = await workflow.step(welcome, input=WelcomeInput(name=input.name), start_to_close_timeout=timedelta(seconds=120))
        log.info("GreetingWorkflow completed", result=result)
        return result




### File: re_act/schedule_workflow.py ###
import asyncio
import time
from restack_ai import Restack

async def main():

    client = Restack()

    workflow_id = f"{int(time.time() * 1000)}-ParentWorkflow"
    runId = await client.schedule_workflow(
        workflow_name="ParentWorkflow",
        workflow_id=workflow_id,
        input={
            "email": "admin@example.com",
            "current_accepted_applicants_count": 10
        }
    )

    await client.get_workflow_result(
        workflow_id=workflow_id,
        run_id=runId
    )

    exit(0)

def run_schedule_workflow():
    asyncio.run(main())

if __name__ == "__main__":
    run_schedule_workflow()

### File: re_act/src/client.py ###
from restack_ai import Restack

client = Restack()

### File: re_act/src/functions/decide.py ###
from restack_ai.function import function, log, FunctionFailure
from dataclasses import dataclass
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

@dataclass
class DecideInput:
    email: str
    current_accepted_applicants_count: int

@function.defn()
async def decide(input: DecideInput):
    try:
        if (os.environ.get("OPENAI_API_KEY") is None):
                raise FunctionFailure("OPENAI_API_KEY is not set", non_retryable=True)
            
        client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

        tools = [
            {
                "type": "function",
                "function": {
                    "name": "accept_applicant",
                    "description": "Accept the applicant",
                    "parameters": {}
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "reject_applicant",
                    "description": "Reject the applicant",
                    "parameters": {}
                }
            }
        ]

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": f"""
                    You are a helpful assistant for event registration that decides if the applicant should be accepted or rejected.
                    """
                },
                {
                    "role": "user",
                    "content": f"""
                    The event is called "Restack AI Summit 2025"
                    Restack is the main sponsor of the event.
                    The applicant has the following email: {input.email}
                    The current number of accepted applicants is: {input.current_accepted_applicants_count}
                    The maximum number of accepted applicants is: 10

                    Decide if the applicant should be accepted or rejected.
                    """,
                }
            ],
            tools=tools
        )
        
        return response.choices[0].message.tool_calls
    except Exception as e:
        log.error("Failed to decide", error=e)
        raise FunctionFailure("Failed to decide", non_retryable=True)



### File: re_act/src/functions/generate_email_content.py ###
from restack_ai.function import function, log, FunctionFailure
from dataclasses import dataclass
from openai import OpenAI
import os
from dotenv import load_dotenv

load_dotenv()

@dataclass
class GenerateEmailInput:
    email_context: str

@function.defn()
async def generate_email_content(input: GenerateEmailInput):

    try:
        client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": f"""
                    You are a helpful assistant that generates short emails based on the provided context.
                    """
                },
                {
                    "role": "user",
                    "content": f"""Generate a short email based on the following context: {input.email_context}
                    """
                }
            ],
            max_tokens=150
        )
        
        return response.choices[0].message.content
    except Exception as e:
        log.error("Failed to generate email content", error=e)
        raise FunctionFailure("Failed to generate email content", non_retryable=True)



### File: re_act/src/functions/send_email.py ###
import sendgrid
from sendgrid.helpers.mail import Mail
from restack_ai.function import function, log, FunctionFailure
from dotenv import load_dotenv
import os
from dataclasses import dataclass

load_dotenv()

@dataclass
class SendEmailInput:
    subject: str
    body: str

@function.defn()
async def send_email(input: SendEmailInput):
    from_email = os.environ.get("FROM_EMAIL")

    if not from_email:
        raise FunctionFailure('FROM_EMAIL is not set', non_retryable=True)
    
    sendgrid_api_key = os.getenv('SENDGRID_API_KEY')

    if not sendgrid_api_key:
        raise FunctionFailure('SENDGRID_API_KEY is not set', non_retryable=True)
    
    message = Mail(
        from_email=from_email,
        to_emails=from_email,
        subject=input.subject,
        plain_text_content=input.body
    )

    try:
        sg = sendgrid.SendGridAPIClient(api_key=sendgrid_api_key)
        sg.send(message)
    except Exception as e:
        log.error("Failed to send email", error=e)
        raise FunctionFailure("Failed to send email", non_retryable=True)


### File: re_act/src/services.py ###
import asyncio
from src.client import client
from src.functions.decide import decide
from src.functions.generate_email_content import generate_email_content
from src.functions.send_email import send_email
from src.workflows.parent_workflow import ParentWorkflow
from src.workflows.child_workflow_a import ChildWorkflowA
from src.workflows.child_workflow_b import ChildWorkflowB

async def main():
    await asyncio.gather(
        client.start_service(
            workflows=[ParentWorkflow, ChildWorkflowA, ChildWorkflowB],
            functions=[decide, generate_email_content, send_email]
        )
    )

def run_services():
    asyncio.run(main())

if __name__ == "__main__":
    run_services()


### File: re_act/src/workflows/child_workflow_a.py ###
from restack_ai.workflow import workflow, log, import_functions
from datetime import timedelta
from dataclasses import dataclass

with import_functions():
    from src.functions.generate_email_content import generate_email_content, GenerateEmailInput
    from src.functions.send_email import send_email, SendEmailInput

@workflow.defn()
class ChildWorkflowA:
    @workflow.run
    async def run(self, email: str):
        log.info(f"Sending email to {email}")

        text = await workflow.step(
            generate_email_content,
            input=GenerateEmailInput(
                email_context="Application was accepted"
            ),
            start_to_close_timeout=timedelta(seconds=120)
        )

        await workflow.step(
            send_email,
            input=SendEmailInput(
                subject="Restack AI Summit 2025",
                body=text
            ),
            start_to_close_timeout=timedelta(seconds=120)
        )


### File: re_act/src/workflows/child_workflow_b.py ###
from restack_ai.workflow import workflow, log, import_functions
from datetime import timedelta
from dataclasses import dataclass

with import_functions():
    from src.functions.generate_email_content import generate_email_content, GenerateEmailInput
    from src.functions.send_email import send_email, SendEmailInput

@workflow.defn()
class ChildWorkflowB:
    @workflow.run
    async def run(self, email: str):
        log.info(f"Sending email to {email}")

        text = await workflow.step(
            generate_email_content,
            input=GenerateEmailInput(
                email_context="Application was rejected"
            ),
            start_to_close_timeout=timedelta(seconds=120)
        )

        await workflow.step(
            send_email,
            input=SendEmailInput(
                subject="Restack AI Summit 2025",
                body=text
            ),
            start_to_close_timeout=timedelta(seconds=120)
        )


### File: re_act/src/workflows/parent_workflow.py ###
from restack_ai.workflow import workflow, log, workflow_info, import_functions
from datetime import timedelta
import json
from dataclasses import dataclass
from .child_workflow_a import ChildWorkflowA
from .child_workflow_b import ChildWorkflowB

with import_functions():
    from src.functions.decide import decide, DecideInput

@dataclass
class ParentWorkflowInput:
    email: str
    current_accepted_applicants_count: int

@workflow.defn()
class ParentWorkflow:
    @workflow.run
    async def run(self, input: ParentWorkflowInput):
        parent_workflow_id = workflow_info().workflow_id

        decide_result = await workflow.step(
            decide,
            input=DecideInput(
                email=input.email,
                current_accepted_applicants_count=input.current_accepted_applicants_count
            ),
            start_to_close_timeout=timedelta(seconds=120)
        )

        decision = decide_result[0]['function']['name']

        child_workflow_result = None
        if decision == "accept_applicant":
            child_workflow_result = await workflow.child_execute(
                ChildWorkflowA,
                workflow_id=f"{parent_workflow_id}-child-a",
                input=input.email
            )
        elif decision == "reject_applicant":
            child_workflow_result = await workflow.child_execute(
                ChildWorkflowB,
                workflow_id=f"{parent_workflow_id}-child-b",
                input=input.email
            )

        return child_workflow_result




